{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment Claims and Housing Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up necessary packagaes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score, auc, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Price Data\n",
    "Load Housing Data from file and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_zHousing_df = pd.read_csv('State_Zhvi_AllHomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "zHousing_df = raw_zHousing_df.drop(['RegionID','RegionType','StateName', 'SizeRank'], axis = 1)\n",
    "zHousing_df = zHousing_df.set_index(['RegionName'])\n",
    "zHousing_df = zHousing_df.T\n",
    "zHousing_df['Date'] = pd.to_datetime(zHousing_df.index)\n",
    "#zHousing_df['month'] = zHousing_df['Date'].dt.month\n",
    "zHousing_df['year'] = zHousing_df['Date'].dt.year\n",
    "zHousing_df['week'] = zHousing_df['Date'].dt.week\n",
    "zHousing_df = zHousing_df.set_index(['year','week'])\n",
    "\n",
    "zHousing_df = zHousing_df.drop(['Date'], axis=1)\n",
    "#zHousing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for individual counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cityHousing_df = pd.read_csv('City_Zhvi_AllHomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_countyHP_df = raw_cityHousing_df.drop(['RegionID','RegionType','RegionName', 'SizeRank','Metro','State'], axis = 1)\n",
    "\n",
    "# get full state name from state housing price datafile\n",
    "state_abbrvMap = raw_zHousing_df[['StateName','RegionName']]\n",
    "countyHP_df = pd.merge(tmp_countyHP_df, state_abbrvMap, how='left', left_on='StateName', right_on='StateName')\n",
    "countyHP_df = countyHP_df.drop(['StateName'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CountyPrices(State):\n",
    "    \n",
    "    df = countyHP_df[countyHP_df['RegionName'] == State]\n",
    "    df = df.drop(['RegionName'], axis=1)\n",
    "    \n",
    "    df = df.set_index(['CountyName'])\n",
    "    df = df.T\n",
    "    df['Date'] = pd.to_datetime(df.index)\n",
    "    df['week'] = df['Date'].dt.week\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df = df.set_index(['year','week'])\n",
    "    \n",
    "    df = df.drop(['Date'], axis=1)\n",
    "    \n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#get_CountyPrices('Michigan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insurance Claims Data\n",
    "\n",
    "Pre-process the insurance claims data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Uinsurance_df = pd.read_csv('r539cy-master.xls.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uinsurance_df = raw_Uinsurance_df\n",
    "Uinsurance_df['Date'] = pd.to_datetime(raw_Uinsurance_df['Filed week ended'])\n",
    "#Uinsurance_df['month'] = Uinsurance_df['Date'].dt.month\n",
    "Uinsurance_df['year'] = Uinsurance_df['Date'].dt.year\n",
    "Uinsurance_df['week'] = Uinsurance_df['Date'].dt.week\n",
    "\n",
    "Uinsurance_df = Uinsurance_df.drop(['Date'], axis=1)\n",
    "# drop the records before 1996, to match the data  that is available from Zillow\n",
    "Uinsurance_df = Uinsurance_df[Uinsurance_df['year'] > 1995]\n",
    "\n",
    "Uinsurance_df['Initial Claims'] = Uinsurance_df['Initial Claims'].str.replace(',', '').astype(int)\n",
    "Uinsurance_df['Continued Claims'] = Uinsurance_df['Continued Claims'].str.replace(',', '').astype(int)\n",
    "Uinsurance_df['Covered Employment'] = Uinsurance_df['Covered Employment'].str.replace(',', '').astype(int)\n",
    "\n",
    "#Uinsurance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform unemployment insurance data into format that can be merged with the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_initclaims_df = pd.pivot_table(Uinsurance_df, values='Initial Claims', index=['year', 'week'], columns=['State'], aggfunc=np.sum)\n",
    "ui_contclaims_df = pd.pivot_table(Uinsurance_df, values='Continued Claims', index=['year', 'week'], columns=['State'], aggfunc=np.sum)\n",
    "ui_rate_df = pd.pivot_table(Uinsurance_df, values='Insured Unemployment Rate', index=['year', 'week'], columns=['State'], aggfunc=np.mean)\n",
    "ui_covEmp_df = pd.pivot_table(Uinsurance_df, values='Covered Employment', index=['year', 'week'], columns=['State'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Data\n",
    "Combine UI and Housing Price data for each State into DataFrame and Engineer training featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_State_df (stateName):    \n",
    "    \n",
    "    tmp_sHousing_df = zHousing_df[stateName]\n",
    "    \n",
    "    df = pd.DataFrame([])\n",
    "    df['Initial Claims'] = ui_initclaims_df[stateName]\n",
    "    df['Continued Claims'] = ui_contclaims_df[stateName]\n",
    "    df['Covered Employment'] = ui_covEmp_df[stateName]\n",
    "    df['Insured Unemployment Rate'] = ui_rate_df[stateName]\n",
    "    \n",
    "    df = pd.merge(df, tmp_sHousing_df, how='inner', left_index=True, right_index=True)\n",
    "    df[stateName].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    df['housingTarget'] = round(df[stateName] / 1000)\n",
    "    df['PrevHousePrc'] = df[stateName].shift(1)\n",
    "    df['MvngAvgPrc15'] = round(df['PrevHousePrc'].rolling(15).mean())\n",
    "    df['MvngAvgPrc30'] = round(df['PrevHousePrc'].rolling(30).mean())\n",
    "    df['MvngAvgPrc02'] = round(df['PrevHousePrc'].rolling(2).sum())\n",
    "    df['PreviousRow']  = df['MvngAvgPrc02'] - df['PrevHousePrc']\n",
    "    df['PercentChnge'] = round((df['PrevHousePrc'] - df['PreviousRow']) / (df['PreviousRow'] + 1), 2)\n",
    "    \n",
    "    \n",
    "    #county_df = get_CountyPrices(stateName)\n",
    "    #df = pd.merge(df, county_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    tmp_arr = []\n",
    "    if stateName in df.columns:\n",
    "        df = df.drop([stateName], axis=1)\n",
    "        \n",
    "        #df[county_df.columns].fillna(method='backfill', inplace=True) \n",
    "        #for county in county_df.columns:\n",
    "        #    tmp_arr = df[county].shift(1)\n",
    "        #    df[county] = tmp_arr\n",
    "            \n",
    "\n",
    "    df.sort_index(ascending=True, inplace=True)  \n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_State_df ('Michigan')\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Test Function\n",
    "Test how strongly the values are co-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_State_Prop (df):\n",
    "    \n",
    "    fin_r = stats.ttest_ind(df['Initial Claims'], df['housingTarget'], equal_var=False)\n",
    "    return fin_r[0], fin_r[1], len(df['housingTarget'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alaska', 0.4454676775539278, 0.3499572609589219, -21.27777777777778, -34.0],\n",
       " ['Michigan',\n",
       "  0.5579743229997542,\n",
       "  0.3106679982693309,\n",
       "  11.303370786516854,\n",
       "  14.000000000000002]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_StateModels():\n",
    "\n",
    "    state_Results = []    \n",
    "    offset = 1\n",
    "    \n",
    "    #states = zHousing_df.columns\n",
    "    states = ['Alaska','Michigan']\n",
    "    \n",
    "    for state in states:\n",
    "\n",
    "        df = get_State_df(state)\n",
    "        \n",
    "        # features\n",
    "        X = df[['Initial Claims','Continued Claims','Insured Unemployment Rate','Covered Employment','PrevHousePrc']]\n",
    "        # target\n",
    "        y = df[['PercentChnge']]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "        #clf = DecisionTreeRegressor(learning_rate = 0.015, random_state = 42, max_depth = 2, n_estimators = 110)\n",
    "        clf = DecisionTreeRegressor(random_state = 42, max_depth = 3)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predicted = clf.predict(X.iloc[-1:]) \n",
    "\n",
    "        #statistic, pvalue, unique_targets = get_State_Prop(X, y)\n",
    "        state_Results.append([state, clf.score(X_train, y_train), clf.score(X_test, y_test), \n",
    "                              #statistic, pvalue, unique_targets, \n",
    "                              y_predicted[0], y.iloc[-1:].to_numpy()[0][0]])\n",
    "        \n",
    "    return state_Results\n",
    "\n",
    "train_StateModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Assessment for GBDT\n",
    "Call the grid search to identify which parameters work best for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best parameter (max. AUC):  {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "Test set Accuracy:  1.0\n",
      "Grid best score (AUC):  0.9968354430379747\n"
     ]
    }
   ],
   "source": [
    "def GBDT_Params(state):\n",
    "    \n",
    "    df = get_State_df(state)\n",
    "    \n",
    "    X = df[['Initial Claims','Continued Claims','Insured Unemployment Rate','Covered Employment','PrevHousePrc']]\n",
    "    y = df[['PercentChnge']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "    grid_values = {'learning_rate':[0.01, 0.015, 0.1, 0.05], 'max_depth': [2, 3], 'n_estimators': [100, 110, 120]}\n",
    "\n",
    "    clf = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 3, random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    grid_gbc_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'accuracy', n_jobs=3, cv=3)\n",
    "    grid_gbc_auc.fit(X_train, y_train)\n",
    "    y_decision_fn_scores_auc = grid_gbc_auc.decision_function(X_test) \n",
    "    y_gbc_predicted = grid_gbc_auc.predict(X_test)\n",
    "\n",
    "    print('Grid best parameter (max. AUC): ', grid_gbc_auc.best_params_)\n",
    "    print('Test set Accuracy: ', accuracy_score(y_test, y_gbc_predicted))\n",
    "    print('Grid best score (AUC): ', grid_gbc_auc.best_score_)  \n",
    "    \n",
    "    return \n",
    "\n",
    "GBDT_Params('Michigan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP CLassifier Model\n",
    "def train_StateMLPMdls():\n",
    "\n",
    "    state_Results = []\n",
    "    factor = 1 #5000\n",
    "    \n",
    "    #states = sorted(zHousing_df.columns)\n",
    "    states = ['Alaska','Michigan']\n",
    "    \n",
    "    for state in states:\n",
    "        \n",
    "        df = get_State_df(state)\n",
    "\n",
    "        X = df.drop(['housingTarget','housingTarget'], axis=1)\n",
    "        y = df[['PercentChnge']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_ftest_scaled = scaler.transform(X.iloc[-1:])\n",
    "        \n",
    "        #mlp_clf = MLPClassifier(hidden_layer_sizes = [50, 50], solver='lbfgs' ).fit(X_train_scaled, y_train)                \n",
    "        mlp_clf = MLPRegressor(hidden_layer_sizes = [50, 50], solver='lbfgs' ).fit(X_train_scaled, y_train)                \n",
    "        y_predicted = mlp_clf.predict(X_ftest_scaled) \n",
    "\n",
    "        #statistic, pvalue, unique_targets = get_State_Prop(X, y)\n",
    "        state_Results.append([state, mlp_clf.score(X_train_scaled, y_train), mlp_clf.score(X_test_scaled, y_test), \n",
    "                              #statistic, pvalue, unique_targets, \n",
    "                              y_predicted[0]*factor, y.iloc[-1:].to_numpy()[0][0]*factor])\n",
    "        \n",
    "    return state_Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>-34.364236</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>13.510740</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2          3     4\n",
       "0    Alaska  0.999923  0.999830 -34.364236 -34.0\n",
       "1  Michigan  0.999957  0.999935  13.510740  14.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_results(model):\n",
    "\n",
    "    if 'mlp' in model:\n",
    "        State_results_df = pd.DataFrame(train_StateMLPMdls())\n",
    "        #State_results_df.columns = ['State','Train Score', 'Test Score', 'Statistic', 'Pvalue', 'Unique Targets','Predicted', 'Actual']\n",
    "        State_results_df.to_csv('stateResults_mlp.csv', index=False)\n",
    "    else: \n",
    "        State_results_df = pd.DataFrame(train_StateModels())\n",
    "        #State_results_df.sort_values(by=['Test Score'], ascending=False, inplace=True)  \n",
    "        #State_results_df.columns = ['State','Train Score', 'Test Score', 'Statistic', 'Pvalue', 'Unique Targets','Predicted', 'Actual']\n",
    "        State_results_df.to_csv('stateResults_gbc.csv', index=False)\n",
    "    \n",
    "    return State_results_df.head()\n",
    "    \n",
    "save_results('mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "GDP\n",
    "#https://apps.bea.gov/regional/Downloadzip.cfm\n",
    "#https://apps.bea.gov/itable/iTable.cfm?ReqID=70&step=1\n",
    "\n",
    "Unemployment\n",
    "#https://oui.doleta.gov/unemploy/claims_arch.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
